---
permalink: /
title: "Chenzhuo Zhao"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% comment %}
If you use a Google Scholar badge/stats plugin, replace the `user=` id below with your Scholar ID.
https://scholar.google.com/citations?user=XXXXXXXXXXX
{% endcomment %}

<span class='anchor' id='about-me'></span>

# üëã About Me

I am **Chenzhuo Zhao**, currently an **M.E. student in Software Engineering** at **Peking University, School of Software & Microelectronics** (2024.09‚Äì2027.07). I received my B.E. in **Software Engineering** from **Northwestern Polytechnical University (NPU), School of Software** (2020.09‚Äì2024.07), ranked **1/313** with **GPA 3.80/4.10**.

My research focuses on **LLM post-training**, including **RFT / RLVR / GRPO-style methods**, **prompt optimization**, and **evaluation** (multi-judge frameworks, benchmarking).

- Email: `cyczzhao@gmail.com`
- Phone: `177-8297-1779`

---
# üíº Career Interests

I am currently interested in the following work directions:

ü§ñ **Large Language Model Applications**: Developing and deploying practical, production ready applications of LLMs across diverse domains, with an emphasis on real world constraints such as latency, cost, reliability, and user experience.

‚öñÔ∏è **Large Language Model Alignment**: Aligning LLM behaviors with human values and intentions to build safer, more reliable systems, covering robustness, controllability, and evaluation of alignment outcomes.

üß© **Agentic Systems for Real World Problem Solving**: Building and iterating on LLM powered agents that can plan, use tools, and execute multi step workflows to solve real user problems end to end, with strong attention to verification, safety, and measurable impact.

---

# üìù Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025</div><img src='images/PMPO.png' alt="paper" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models**

**Chenzhuo Zhao**, Ziqian Liu, Xinda Wang, Junting Lu, Chaoyi Ruan

[**Paper**](#)  <strong><span class='show_paper_citations' data=''></span></strong>

- Propose **PMPO**, a token-level scoring & reward-model-driven prompt optimization framework.
- Unifies *low-quality prompt diagnosis ‚Üí sample generation ‚Üí policy rewrite* for both small and large LMs.
- Improves accuracy on BBH-style tasks and boosts win-rate on AlpacaEval 2.0.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/tase.png' alt="paper" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**TASE: Token Awareness and Structured Evaluation for Multilingual Language Models**

**Chenzhuo Zhao**, Xinda Wang, Yue Huang, Junting Lu, Ziqian Liu

[**Paper**](#) \| [**Code**](#) <strong><span class='show_paper_citations' data=''></span></strong>

- Introduce a structured evaluation benchmark for multilingual LMs, addressing tokenization-induced bias.
- Build a scalable data-generation pipeline and a **36k** test set.
- Analyze mechanisms with GRPO-trained Qwen2.5-14B.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2026 (Under Review)</div><img src='images/ARPO.png' alt="paper" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**ARPO: Answer-Refined Policy Optimization for Learning from Hard Instances in Group-Relative RLVR**

**ChenZhuo Zhao**, Pu Zhao, Fangkai Yang, Lu Wang, Qibin Wang, Liqun Li, Xinda Wang, Ran Jia, Xu Chen, Junting Lu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang

[**Paper**](#)  <strong><span class='show_paper_citations' data=''></span></strong>

- Propose **ARPO** to improve learning from hard instances under group-relative rewards in RLVR.
- Combine answer-refined prompting with off-policy shaping to mitigate training stalls (e.g., all-zero groups).
- Improve pass@1 on Qwen2.5-Math-7B (e.g., **37.61 ‚Üí 40.38**).

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2026 (Under Review)</div><img src='images/gmpo.png' alt="paper" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Gradient-Guided Multi-Judge Prompt Optimization**

**ChenZhuo Zhao**, Xinda Wang, Pu Zhao, Yue Huang, Junting Lu, Ziqian Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang

[**Paper**](#) \| [**Code**](#) <strong><span class='show_paper_citations' data=''></span></strong>

- Use multi-judge feedback to reduce overfitting to a single evaluator and improve transferability.
- Introduce gradient-guided refinement for more stable prompt updates.
- Achieve consistent gains on math reasoning, language understanding, and instruction following tasks.

</div>
</div>


- **PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models**, **Chenzhuo Zhao**, Ziqian Liu, Xinda Wang, Junting Lu, Chaoyi Ruan **EMNLP 2025**
- **TASE: Token Awareness and Structured Evaluation for Multilingual Language Models**, **Chenzhuo Zhao**, Xinda Wang, Yue Huang, Junting Lu, Ziqian Liu **AAAI 2025**
- **ARPO: Answer-Refined Policy Optimization for Learning from Hard Instances in Group-Relative RLVR**, **ChenZhuo Zhao**, Pu Zhao, Fangkai Yang, Lu Wang, Qibin Wang, Liqun Li, Xinda Wang, Ran Jia, Xu Chen, Junting Lu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang **ACL 2026 (Under Review)**
- **Gradient-Guided Multi-Judge Prompt Optimization**, **ChenZhuo Zhao**, Xinda Wang, Pu Zhao, Yue Huang, Junting Lu, Ziqian Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang **ACL 2026 (Under Review)**
- **EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation**, Xinda Wang, Zhengxu Hou, Yangshijie Zhang, yanbingren, Jialin Liu, **ChenZhuo Zhao**, Zhibo Yang, Bin-Bin Yang, Feng Xiao **ACL 2026 (Under Review)**
- **Triviality Corrected Endogenous Reward**, Xinda Wang, Zhengxu Hou, Yangshijie Zhang, yanbingren, Jialin Liu, **ChenZhuo Zhao**, Zhibo Yang, Bin-Bin Yang, Feng Xiao **ACL 2026 (Under Review)**
- **MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction**, Yue Huang, Yanyuan Chen, Dexuan Xu, **ChenZhuo Zhao**, Weihua Yue, Yu Huang **ACL 2026 (Under Review)**
- **DisRec: Intra-Visit Disease Diagnosis as Recommendation**, Xinda Wang, Hongzhi Liu, **ChenZhuo Zhao**, wenhao zhang **IJCAI 2026 (Under Review)**

Additional manuscripts are in progress, with planned submissions to **ICML** (second author on one paper),**EMNLP** (first author on one paper), and **ACM Multimedia** (ACMMM) (first author on one paper).

---

# üéñ Honors and Awards
- **National Scholarship** (two consecutive years)
- **Xiaomi Special Scholarship**
- **Outstanding Student Role Model (Nominee)**, NPU
- **First-class Scholarship** (three consecutive years)
- **Outstanding Student**, **Outstanding Communist Youth League Cadre**
- **National Undergraduate Innovation & Entrepreneurship Project** (excellent completion, two consecutive years)
- Tencent ‚Äú**Supernova**‚Äù Program ‚Äî **Excellent Completion**

---

# üìñ Education
- **Peking University** ‚Äî M.E., Software Engineering (School of Software & Microelectronics)  
  *Sep 2024 ‚Äì Jul 2027*  
  Research: LLM post-training

- **Northwestern Polytechnical University (NPU)** ‚Äî B.E., Software Engineering (School of Software)  
  *Sep 2020 ‚Äì Jul 2024*  
  GPA: 3.80/4.10 | Rank: 1/313

---

# üíª Experience

### Microsoft ‚Äî LLM Research Intern, STCA
*Sep 2025 ‚Äì Present*  
- Work on post-training with verifiable feedback, tackling **sparse learning signals**, **training instability**, and **upper-bound bottlenecks**. Responsible for training design, experiments/ablations, evaluation, and diagnostics.  
- **Learning-signal enhancement**: explore ARPO-style strategies to better exploit supervision/reference information under strict verification rewards without changing the main framework.  
- **Process-signal / credit assignment**: propose finer-grained credit assignment using structured intermediate reasoning states to better control advantage/reward allocation.

### Xiaohongshu ‚Äî LLM Algorithm Intern, AI Platform
*Feb 2025 ‚Äì Jul 2025*  
- Improve search Query understanding under sparse/noisy user intent signals; focus on an interpretable, end-to-end pipeline from **Query understanding ‚Üí recall enhancement ‚Üí review/risk identification**, and ship optimizations online.  
- **Query understanding & recall**: iterate QueryNER with online incremental learning and hot updates; low-resource multi-domain hard-example mining and data augmentation; adopt LLM-as-Judge for data quality; SFT+GRPO on Qwen2.5-14B, reaching ~82.1% recognition accuracy and >10% gains in online/offline F1.  
- **Ecosystem review & routing**: design an 8-step structured reasoning template and multi-metric agreement checks; SFT+GRPO based on Gemma3-27B, boosting routing task F1 from 0.56‚Üí0.71 with improved stability.  
- **Training platform & experimentation**: support engine iterations and large-scale experiments; run DeepSeek 671B LoRA on 64√óH20; completed 22,400 experiment groups to support performance/stability optimization and business metric tracking.

### Tencent ‚Äî Computer Vision Intern, YouTu Lab
*Apr 2024 ‚Äì Jan 2025*  
- Built and deployed rPPG-based face anti-spoofing for payment scenarios: method design, scenario validation, on-device deployment, and evaluation.  
- **Paper**: propose **EST-rPPG** to address unstable short-window rPPG signals and 3D mask robustness; reliable detection with ~0.13s window (first-author submission).  
- **Transfer & deployment**: extend rPPG liveness to palm liveness recognition; refactor and deploy the full pipeline; achieved 99.99% accuracy on self-collected data.  
- **Competition & patents**: 6th place in IJCAI RCPSS unsupervised challenge; filed 3 invention patents covering core method and engineering components.

---

{% comment %}
Optional: keep Scholar stats here if you want:
- Google Scholar: https://scholar.google.com/citations?user=XXXXXXXXXXX
{% endcomment %}
